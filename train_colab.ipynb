{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Snake DQN Training (Colab)\n",
        "\n",
        "Use this notebook on Google Colab to train the Snake DQN and download the checkpoint. Steps:\n",
        "1. Set your git repository URL in the clone cell below.\n",
        "2. Run the setup/install cell to clone the repo and install dependencies.\n",
        "3. Adjust hyperparameters in the args cell if desired.\n",
        "4. Run training.\n",
        "5. Download the trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clone command (token masked): git clone --depth 1 https://github.com/SyntheticVis-Umut/Snake.git /content/Snake\n",
            "CWD: /content/Snake\n",
            "Repository cloned and ready!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository (set your git URL)\n",
        "import os\n",
        "\n",
        "REPO_URL = os.environ.get(\"SNAKE_REPO_URL\", \"https://github.com/SyntheticVis-Umut/Snake.git\")  # change if using a fork\n",
        "GITHUB_TOKEN = os.environ.get(\"SNAKE_GITHUB_TOKEN\") or os.environ.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# Clone and setup\n",
        "import shutil\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "if not REPO_URL:\n",
        "    raise ValueError(\"Set REPO_URL (or env SNAKE_REPO_URL) before running this cell.\")\n",
        "\n",
        "if os.path.exists('/content/Snake'):\n",
        "    shutil.rmtree('/content/Snake')\n",
        "\n",
        "clone_url = REPO_URL\n",
        "masked_url = REPO_URL\n",
        "if GITHUB_TOKEN and REPO_URL.startswith(\"https://github.com/\"):\n",
        "    # Inject token for private repos (note: token will appear in Colab logs)\n",
        "    clone_url = REPO_URL.replace(\"https://\", f\"https://{GITHUB_TOKEN}@\")\n",
        "    masked_url = REPO_URL.replace(\"https://\", \"https://<TOKEN>@\")\n",
        "    print(\"Using token from env for clone.\")\n",
        "\n",
        "clone_cmd = [\"git\", \"clone\", \"--depth\", \"1\", clone_url, \"/content/Snake\"]\n",
        "print(\"Clone command (token masked):\", \" \".join(clone_cmd).replace(clone_url, masked_url))\n",
        "result = subprocess.run(clone_cmd, capture_output=True, text=True)\n",
        "if result.returncode != 0:\n",
        "    print(\"git clone stdout:\\n\", result.stdout)\n",
        "    print(\"git clone stderr:\\n\", result.stderr)\n",
        "    sys.exit(\"git clone failed; check REPO_URL, token (if private), and permissions\")\n",
        "\n",
        "os.chdir('/content/Snake')\n",
        "\n",
        "# Install deps (CUDA wheels on Colab are handled automatically by torch)\n",
        "%pip install -q pygame torch numpy tqdm\n",
        "\n",
        "print('CWD:', os.getcwd())\n",
        "print('Repository cloned and ready!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "Imports ok\n"
          ]
        }
      ],
      "source": [
        "# Quick import test\n",
        "from train import train\n",
        "from types import SimpleNamespace\n",
        "print('Imports ok')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure hyperparameters\n",
        "args = SimpleNamespace(\n",
        "    episodes=100000,\n",
        "    max_steps=1000,\n",
        "    buffer_size=50000,\n",
        "    batch_size=512,\n",
        "    gamma=0.99,\n",
        "    lr=5e-4,\n",
        "    eps_start=0.5,\n",
        "    eps_end=0.02,\n",
        "    eps_decay=5000,\n",
        "    target_update=1000,\n",
        "    warmup=2000,\n",
        "    grid=(20, 20),\n",
        "    seed=42,\n",
        "    save_path=\"models/dqn_snake_colab.pt\",\n",
        "    resume=None,  # set to a checkpoint path to continue training\n",
        "    device=\"cuda\",  # force GPU (A100 on Colab); use \"auto\" to fall back\n",
        "    grad_clip=1.0,  # gradient clipping for stability; set <=0 to disable\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA device: NVIDIA A100-SXM4-80GB\n",
            "Episode 1/100000 Reward: -2.00 Epsilon: 0.498 Best: -2.00\n",
            "Episode 10/100000 Reward: -3.80 Epsilon: 0.470 Best: -2.00\n",
            "Episode 20/100000 Reward: -11.90 Epsilon: 0.444 Best: -2.00\n",
            "Episode 30/100000 Reward: -12.40 Epsilon: 0.421 Best: -2.00\n",
            "Episode 40/100000 Reward: -13.80 Epsilon: 0.393 Best: -2.00\n",
            "Episode 50/100000 Reward: -12.50 Epsilon: 0.377 Best: -2.00\n",
            "Episode 60/100000 Reward: -12.50 Epsilon: 0.366 Best: -2.00\n",
            "Episode 70/100000 Reward: -11.10 Epsilon: 0.352 Best: -2.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/Snake/src/dqn.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  lambda x: torch.tensor(x, device=device),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 80/100000 Reward: -22.60 Epsilon: 0.318 Best: -2.00\n",
            "Episode 90/100000 Reward: -21.10 Epsilon: 0.278 Best: -2.00\n",
            "Episode 100/100000 Reward: -13.00 Epsilon: 0.240 Best: 8.10\n",
            "Episode 110/100000 Reward: 7.50 Epsilon: 0.206 Best: 53.40\n",
            "Episode 120/100000 Reward: 79.30 Epsilon: 0.174 Best: 79.30\n",
            "Episode 130/100000 Reward: 60.60 Epsilon: 0.149 Best: 79.30\n",
            "Episode 140/100000 Reward: 54.10 Epsilon: 0.136 Best: 79.30\n",
            "Episode 150/100000 Reward: 15.10 Epsilon: 0.118 Best: 114.50\n",
            "Episode 160/100000 Reward: 169.80 Epsilon: 0.092 Best: 169.80\n",
            "Episode 170/100000 Reward: 126.40 Epsilon: 0.068 Best: 192.80\n",
            "Episode 180/100000 Reward: 91.10 Epsilon: 0.048 Best: 236.40\n",
            "Episode 190/100000 Reward: 220.70 Epsilon: 0.038 Best: 236.40\n",
            "Episode 200/100000 Reward: 99.30 Epsilon: 0.030 Best: 236.40\n",
            "Episode 210/100000 Reward: 122.10 Epsilon: 0.026 Best: 314.80\n",
            "Episode 220/100000 Reward: 208.90 Epsilon: 0.023 Best: 329.80\n",
            "Episode 230/100000 Reward: 123.30 Epsilon: 0.022 Best: 329.80\n",
            "Episode 240/100000 Reward: 174.40 Epsilon: 0.021 Best: 329.80\n",
            "Episode 250/100000 Reward: 123.40 Epsilon: 0.021 Best: 329.80\n",
            "Episode 260/100000 Reward: 84.30 Epsilon: 0.020 Best: 329.80\n",
            "Episode 270/100000 Reward: 303.20 Epsilon: 0.020 Best: 329.80\n",
            "Episode 280/100000 Reward: 151.70 Epsilon: 0.020 Best: 348.20\n",
            "Episode 290/100000 Reward: 245.50 Epsilon: 0.020 Best: 348.20\n",
            "Episode 300/100000 Reward: 98.20 Epsilon: 0.020 Best: 348.20\n",
            "Episode 310/100000 Reward: 119.50 Epsilon: 0.020 Best: 375.90\n",
            "Episode 320/100000 Reward: 267.80 Epsilon: 0.020 Best: 375.90\n",
            "Episode 330/100000 Reward: 14.80 Epsilon: 0.020 Best: 375.90\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3696848000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training done, saved to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Snake/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 loss = optimize_model(\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Snake/train.py\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(policy_net, target_net, memory, optimizer, criterion, batch_size, gamma, device)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m ):\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Snake/src/dqn.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         state, action, reward, next_state, done = map(\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Snake/src/dqn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         state, action, reward, next_state, done = map(\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train\n",
        "train(args)\n",
        "print('Training done, saved to', args.save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the trained model (Colab)\n",
        "from google.colab import files\n",
        "files.download(args.save_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
