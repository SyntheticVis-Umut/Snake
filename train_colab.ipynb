{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Snake DQN Training (Colab)\n",
        "\n",
        "Use this notebook on Google Colab to train the Snake DQN and download the checkpoint. Steps:\n",
        "1. Set your git repository URL in the clone cell below.\n",
        "2. Run the setup/install cell to clone the repo and install dependencies.\n",
        "3. Adjust hyperparameters in the args cell if desired.\n",
        "4. Run training.\n",
        "5. Download the trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clone command (token masked): git clone --depth 1 https://github.com/SyntheticVis-Umut/Snake.git /content/Snake\n",
            "CWD: /content/Snake\n",
            "Repository cloned and ready!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository (set your git URL)\n",
        "import os\n",
        "\n",
        "REPO_URL = os.environ.get(\"SNAKE_REPO_URL\", \"https://github.com/SyntheticVis-Umut/Snake.git\")  # change if using a fork\n",
        "GITHUB_TOKEN = os.environ.get(\"SNAKE_GITHUB_TOKEN\") or os.environ.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# Clone and setup\n",
        "import shutil\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "if not REPO_URL:\n",
        "    raise ValueError(\"Set REPO_URL (or env SNAKE_REPO_URL) before running this cell.\")\n",
        "\n",
        "if os.path.exists('/content/Snake'):\n",
        "    shutil.rmtree('/content/Snake')\n",
        "\n",
        "clone_url = REPO_URL\n",
        "masked_url = REPO_URL\n",
        "if GITHUB_TOKEN and REPO_URL.startswith(\"https://github.com/\"):\n",
        "    # Inject token for private repos (note: token will appear in Colab logs)\n",
        "    clone_url = REPO_URL.replace(\"https://\", f\"https://{GITHUB_TOKEN}@\")\n",
        "    masked_url = REPO_URL.replace(\"https://\", \"https://<TOKEN>@\")\n",
        "    print(\"Using token from env for clone.\")\n",
        "\n",
        "clone_cmd = [\"git\", \"clone\", \"--depth\", \"1\", clone_url, \"/content/Snake\"]\n",
        "print(\"Clone command (token masked):\", \" \".join(clone_cmd).replace(clone_url, masked_url))\n",
        "result = subprocess.run(clone_cmd, capture_output=True, text=True)\n",
        "if result.returncode != 0:\n",
        "    print(\"git clone stdout:\\n\", result.stdout)\n",
        "    print(\"git clone stderr:\\n\", result.stderr)\n",
        "    sys.exit(\"git clone failed; check REPO_URL, token (if private), and permissions\")\n",
        "\n",
        "os.chdir('/content/Snake')\n",
        "\n",
        "# Install deps (CUDA wheels on Colab are handled automatically by torch)\n",
        "%pip install -q pygame torch numpy tqdm\n",
        "\n",
        "print('CWD:', os.getcwd())\n",
        "print('Repository cloned and ready!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "Imports ok\n"
          ]
        }
      ],
      "source": [
        "# Quick import test\n",
        "from train import train\n",
        "from types import SimpleNamespace\n",
        "print('Imports ok')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure hyperparameters\n",
        "args = SimpleNamespace(\n",
        "    episodes=20000,\n",
        "    max_steps=1000,\n",
        "    buffer_size=100000,\n",
        "    batch_size=256,\n",
        "    gamma=0.995,\n",
        "    lr=3e-4,\n",
        "    eps_start=0.6,\n",
        "    eps_end=0.01,\n",
        "    eps_decay=8000,\n",
        "    target_update=500,\n",
        "    warmup=4000,\n",
        "    grid=(20, 20),\n",
        "    seed=42,\n",
        "    save_path=\"models/dqn_snake_colab.pt\",\n",
        "    resume=None,  # set to a checkpoint path to continue training\n",
        "    device=\"cuda\",  # force GPU (A100 on Colab); use \"auto\" to fall back\n",
        "    grad_clip=1.0,  # gradient clipping for stability; set <=0 to disable\n",
        "    double_dqn=True,  # use Double DQN targets for better stability\n",
        "    eval_every=100,  # run greedy eval every N episodes (0 disables)\n",
        "    eval_episodes=10,  # number of greedy episodes per eval\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA device: NVIDIA A100-SXM4-80GB\n",
            "Episode 1/20000 Reward: -11.60 Epsilon: 0.599 Best: -11.60\n",
            "Episode 10/20000 Reward: -14.60 Epsilon: 0.577 Best: -11.30\n",
            "Episode 20/20000 Reward: -12.10 Epsilon: 0.555 Best: -11.20\n",
            "Episode 30/20000 Reward: -15.90 Epsilon: 0.530 Best: -8.30\n",
            "Episode 40/20000 Reward: -14.30 Epsilon: 0.515 Best: -8.30\n",
            "Episode 50/20000 Reward: -12.10 Epsilon: 0.496 Best: -8.30\n",
            "Episode 60/20000 Reward: -2.90 Epsilon: 0.477 Best: -2.90\n",
            "Episode 70/20000 Reward: -14.40 Epsilon: 0.456 Best: -2.90\n",
            "Episode 80/20000 Reward: -11.20 Epsilon: 0.441 Best: -2.90\n",
            "Episode 90/20000 Reward: -13.20 Epsilon: 0.427 Best: -2.90\n",
            "Episode 100/20000 Reward: -13.10 Epsilon: 0.411 Best: -2.90\n",
            "Episode 110/20000 Reward: -12.10 Epsilon: 0.395 Best: -2.90\n",
            "Episode 120/20000 Reward: -15.80 Epsilon: 0.380 Best: -2.90\n",
            "Episode 130/20000 Reward: -20.70 Epsilon: 0.361 Best: -2.90\n",
            "Episode 140/20000 Reward: -15.30 Epsilon: 0.338 Best: 5.20\n",
            "Episode 150/20000 Reward: -1.70 Epsilon: 0.322 Best: 5.20\n",
            "Episode 160/20000 Reward: 16.80 Epsilon: 0.299 Best: 16.80\n",
            "Episode 170/20000 Reward: 5.00 Epsilon: 0.279 Best: 16.80\n",
            "Episode 180/20000 Reward: -2.50 Epsilon: 0.265 Best: 16.80\n",
            "Episode 190/20000 Reward: 18.10 Epsilon: 0.251 Best: 18.10\n",
            "Episode 200/20000 Reward: 47.40 Epsilon: 0.223 Best: 47.40\n",
            "Episode 210/20000 Reward: 0.40 Epsilon: 0.208 Best: 47.40\n",
            "Episode 220/20000 Reward: -11.80 Epsilon: 0.194 Best: 70.60\n",
            "Episode 230/20000 Reward: 5.70 Epsilon: 0.176 Best: 71.10\n",
            "Episode 240/20000 Reward: 50.90 Epsilon: 0.159 Best: 100.20\n",
            "Episode 250/20000 Reward: 61.60 Epsilon: 0.137 Best: 106.00\n",
            "Episode 260/20000 Reward: 64.50 Epsilon: 0.125 Best: 106.00\n",
            "Episode 270/20000 Reward: 34.80 Epsilon: 0.116 Best: 112.40\n",
            "Episode 280/20000 Reward: 81.00 Epsilon: 0.104 Best: 112.40\n",
            "Episode 290/20000 Reward: -2.40 Epsilon: 0.087 Best: 213.20\n",
            "Episode 300/20000 Reward: 145.60 Epsilon: 0.072 Best: 213.20\n",
            "Episode 310/20000 Reward: 24.70 Epsilon: 0.057 Best: 297.00\n",
            "Episode 320/20000 Reward: 133.30 Epsilon: 0.048 Best: 297.00\n",
            "Episode 330/20000 Reward: 112.90 Epsilon: 0.037 Best: 297.00\n",
            "Episode 340/20000 Reward: 15.40 Epsilon: 0.031 Best: 297.00\n",
            "Episode 350/20000 Reward: 110.90 Epsilon: 0.026 Best: 297.00\n",
            "Episode 360/20000 Reward: 266.40 Epsilon: 0.021 Best: 297.00\n",
            "Episode 370/20000 Reward: 290.20 Epsilon: 0.018 Best: 297.00\n",
            "Episode 380/20000 Reward: 186.70 Epsilon: 0.015 Best: 297.00\n",
            "Episode 390/20000 Reward: 142.30 Epsilon: 0.013 Best: 315.70\n",
            "Episode 400/20000 Reward: 196.50 Epsilon: 0.012 Best: 328.80\n",
            "Episode 410/20000 Reward: 120.90 Epsilon: 0.011 Best: 378.60\n",
            "Episode 420/20000 Reward: 158.40 Epsilon: 0.011 Best: 378.60\n",
            "Episode 430/20000 Reward: 274.40 Epsilon: 0.011 Best: 399.20\n",
            "Episode 440/20000 Reward: 83.50 Epsilon: 0.010 Best: 399.20\n",
            "Episode 450/20000 Reward: 193.50 Epsilon: 0.010 Best: 399.20\n",
            "Episode 460/20000 Reward: 172.50 Epsilon: 0.010 Best: 399.20\n",
            "Episode 470/20000 Reward: 58.30 Epsilon: 0.010 Best: 399.20\n",
            "Episode 480/20000 Reward: 160.50 Epsilon: 0.010 Best: 399.20\n",
            "Episode 490/20000 Reward: 201.20 Epsilon: 0.010 Best: 399.20\n",
            "Episode 500/20000 Reward: 151.10 Epsilon: 0.010 Best: 399.20\n",
            "Episode 510/20000 Reward: 115.40 Epsilon: 0.010 Best: 399.20\n",
            "Episode 520/20000 Reward: 190.80 Epsilon: 0.010 Best: 399.20\n",
            "Episode 530/20000 Reward: 286.50 Epsilon: 0.010 Best: 399.20\n",
            "Episode 540/20000 Reward: 67.70 Epsilon: 0.010 Best: 399.20\n",
            "Episode 550/20000 Reward: 415.30 Epsilon: 0.010 Best: 415.30\n",
            "Episode 560/20000 Reward: 52.40 Epsilon: 0.010 Best: 415.30\n",
            "Episode 570/20000 Reward: 274.70 Epsilon: 0.010 Best: 415.30\n",
            "Episode 580/20000 Reward: 298.80 Epsilon: 0.010 Best: 415.30\n",
            "Episode 590/20000 Reward: 82.40 Epsilon: 0.010 Best: 415.30\n",
            "Episode 600/20000 Reward: 173.50 Epsilon: 0.010 Best: 415.30\n",
            "Episode 610/20000 Reward: 70.10 Epsilon: 0.010 Best: 415.30\n",
            "Episode 620/20000 Reward: 322.90 Epsilon: 0.010 Best: 415.30\n",
            "Episode 630/20000 Reward: 256.30 Epsilon: 0.010 Best: 415.30\n",
            "Episode 640/20000 Reward: 235.90 Epsilon: 0.010 Best: 415.30\n",
            "Episode 650/20000 Reward: 140.80 Epsilon: 0.010 Best: 415.30\n",
            "Episode 660/20000 Reward: 104.30 Epsilon: 0.010 Best: 415.30\n",
            "Episode 670/20000 Reward: 98.50 Epsilon: 0.010 Best: 415.30\n",
            "Episode 680/20000 Reward: 92.80 Epsilon: 0.010 Best: 415.30\n",
            "Episode 690/20000 Reward: 252.30 Epsilon: 0.010 Best: 415.30\n",
            "Episode 700/20000 Reward: 214.10 Epsilon: 0.010 Best: 415.30\n",
            "Episode 710/20000 Reward: 269.10 Epsilon: 0.010 Best: 415.30\n",
            "Episode 720/20000 Reward: 105.10 Epsilon: 0.010 Best: 415.30\n",
            "Episode 730/20000 Reward: 202.00 Epsilon: 0.010 Best: 415.30\n",
            "Episode 740/20000 Reward: 152.30 Epsilon: 0.010 Best: 415.30\n",
            "Episode 750/20000 Reward: 205.30 Epsilon: 0.010 Best: 415.30\n",
            "Episode 760/20000 Reward: 17.80 Epsilon: 0.010 Best: 415.30\n",
            "Episode 770/20000 Reward: 167.50 Epsilon: 0.010 Best: 415.30\n",
            "Episode 780/20000 Reward: 247.10 Epsilon: 0.010 Best: 415.30\n",
            "Episode 790/20000 Reward: 87.90 Epsilon: 0.010 Best: 415.30\n",
            "Episode 800/20000 Reward: 220.40 Epsilon: 0.010 Best: 415.30\n",
            "Episode 810/20000 Reward: 336.20 Epsilon: 0.010 Best: 415.30\n",
            "Episode 820/20000 Reward: 264.80 Epsilon: 0.010 Best: 415.30\n",
            "Episode 830/20000 Reward: 112.70 Epsilon: 0.010 Best: 439.40\n",
            "Episode 840/20000 Reward: 201.70 Epsilon: 0.010 Best: 439.40\n",
            "Episode 850/20000 Reward: 183.40 Epsilon: 0.010 Best: 439.40\n",
            "Episode 860/20000 Reward: 96.20 Epsilon: 0.010 Best: 442.00\n",
            "Episode 870/20000 Reward: 287.20 Epsilon: 0.010 Best: 442.00\n",
            "Episode 880/20000 Reward: 161.50 Epsilon: 0.010 Best: 442.00\n",
            "Episode 890/20000 Reward: 93.80 Epsilon: 0.010 Best: 442.00\n",
            "Episode 900/20000 Reward: 229.60 Epsilon: 0.010 Best: 442.00\n",
            "Episode 910/20000 Reward: 146.10 Epsilon: 0.010 Best: 442.00\n",
            "Episode 920/20000 Reward: 145.70 Epsilon: 0.010 Best: 442.00\n",
            "Episode 930/20000 Reward: 185.80 Epsilon: 0.010 Best: 442.00\n",
            "Episode 940/20000 Reward: 121.10 Epsilon: 0.010 Best: 442.00\n",
            "Episode 950/20000 Reward: 146.00 Epsilon: 0.010 Best: 442.00\n",
            "Episode 960/20000 Reward: 85.20 Epsilon: 0.010 Best: 442.00\n",
            "Episode 970/20000 Reward: 190.40 Epsilon: 0.010 Best: 442.00\n",
            "Episode 980/20000 Reward: 59.30 Epsilon: 0.010 Best: 442.00\n",
            "Episode 990/20000 Reward: 93.50 Epsilon: 0.010 Best: 442.00\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3696848000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training done, saved to'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Snake/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 loss = optimize_model(\n\u001b[0m\u001b[1;32m    119\u001b[0m                     \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Snake/train.py\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(policy_net, target_net, memory, optimizer, criterion, batch_size, gamma, device, grad_clip, double_dqn)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Optimizer.step#{self.__class__.__name__}.step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m                 \u001b[0;31m# call optimizer step pre hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 for pre_hook in chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m             \u001b[0;31m# When any inputs are FakeScriptObject, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;31m# skip c++ dispatcher and dispatch in python through _get_dispatch of python_dispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m_must_dispatch_in_python\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m     return pytree.tree_any(\n\u001b[0m\u001b[1;32m   1130\u001b[0m         lambda obj: isinstance(\n\u001b[1;32m   1131\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_class_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFakeScriptObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_any\u001b[0;34m(pred, tree, is_leaf)\u001b[0m\n\u001b[1;32m   1634\u001b[0m ) -> bool:\n\u001b[1;32m   1635\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;31m# Recursively flatten the children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_pytrees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtree_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0mnode_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_node_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0mflatten_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUPPORTED_NODES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mchild_pytrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;31m# Recursively flatten the children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36m_dict_flatten\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_dict_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train\n",
        "train(args)\n",
        "print('Training done, saved to', args.save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual greedy evaluation of the latest checkpoint\n",
        "import torch\n",
        "from src.dqn import QNetwork\n",
        "from src.env import SnakeEnv\n",
        "from train import evaluate_policy\n",
        "\n",
        "ckpt = torch.load(args.save_path, map_location=args.device)\n",
        "\n",
        "env = SnakeEnv(grid_size=tuple(args.grid), render_mode=None, seed=args.seed + 999)\n",
        "state_dim = env.reset().shape[0]\n",
        "action_dim = len(env.ACTIONS)\n",
        "policy_net = QNetwork(state_dim, action_dim).to(args.device)\n",
        "policy_net.load_state_dict(ckpt[\"policy_state_dict\"])\n",
        "\n",
        "mean_r, median_r, max_r, std_r = evaluate_policy(\n",
        "    policy_net, env, episodes=20, device=args.device, max_steps=args.max_steps\n",
        ")\n",
        "print({\"mean\": mean_r, \"median\": median_r, \"max\": max_r, \"std\": std_r})\n",
        "\n",
        "env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the trained model (Colab)\n",
        "from google.colab import files\n",
        "files.download(args.save_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
