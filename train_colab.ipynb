{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake DQN Training (Colab)\n",
    "\n",
    "Use this notebook on Google Colab to train the Snake DQN and download the checkpoint. Steps:\n",
    "1. Set your git repository URL in the clone cell below.\n",
    "2. Run the setup/install cell to clone the repo and install dependencies.\n",
    "3. Adjust hyperparameters in the args cell if desired.\n",
    "4. Run training.\n",
    "5. Download the trained model.\n",
    "\n",
    "## New Features (Latest Update):\n",
    "- **CNN-based observations**: Uses image input (3 channels: body, head, food) instead of 11 boolean features - better at learning spatial patterns and avoiding self-traps\n",
    "- **N-step returns**: Multi-step learning for longer-term planning (default: 5 steps)\n",
    "- **Double DQN**: Already enabled for better stability\n",
    "- **Huber loss + gradient clipping**: For stable training\n",
    "- **MCTS Planning** (NEW!): After training, use `--planner mcts` in `watch_agent.py` to perform Monte Carlo Tree Search with 10-100+ step lookahead for smarter decisions\n",
    "\n",
    "Default settings use CNN + 1-step for stability. Change `observation_type=\"features\"` to use the old approach.\n",
    "\n",
    "## Using Planning for Smarter Play:\n",
    "After training, you can use the trained model with lookahead planning:\n",
    "- **MCTS**: `python watch_agent.py --model models/dqn_snake_colab.pt --planner mcts --mcts-simulations 200 --mcts-depth 100`\n",
    "- **Simple Lookahead**: `python watch_agent.py --model models/dqn_snake_colab.pt --planner lookahead --lookahead-depth 20`\n",
    "- **Greedy (no planning)**: `python watch_agent.py --model models/dqn_snake_colab.pt --planner none`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clone command (token masked): git clone --depth 1 https://github.com/SyntheticVis-Umut/Snake.git /content/Snake\n",
      "CWD: /content/Snake\n",
      "Repository cloned and ready!\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('✓ Google Drive mounted at /content/drive')\n",
    "\n",
    "# Clone the repository (set your git URL)\n",
    "import os\n",
    "\n",
    "REPO_URL = os.environ.get(\"SNAKE_REPO_URL\", \"https://github.com/SyntheticVis-Umut/Snake.git\")  # change if using a fork\n",
    "GITHUB_TOKEN = os.environ.get(\"SNAKE_GITHUB_TOKEN\") or os.environ.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Clone and setup\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if not REPO_URL:\n",
    "    raise ValueError(\"Set REPO_URL (or env SNAKE_REPO_URL) before running this cell.\")\n",
    "\n",
    "# Ensure /content directory exists (should exist in Colab, but check anyway)\n",
    "os.makedirs('/content', exist_ok=True)\n",
    "\n",
    "if os.path.exists('/content/Snake'):\n",
    "    shutil.rmtree('/content/Snake')\n",
    "\n",
    "clone_url = REPO_URL\n",
    "masked_url = REPO_URL\n",
    "if GITHUB_TOKEN and REPO_URL.startswith(\"https://github.com/\"):\n",
    "    # Inject token for private repos (note: token will appear in Colab logs)\n",
    "    clone_url = REPO_URL.replace(\"https://\", f\"https://{GITHUB_TOKEN}@\")\n",
    "    masked_url = REPO_URL.replace(\"https://\", \"https://<TOKEN>@\")\n",
    "    print(\"Using token from env for clone.\")\n",
    "\n",
    "clone_cmd = [\"git\", \"clone\", \"--depth\", \"1\", clone_url, \"/content/Snake\"]\n",
    "print(\"Clone command (token masked):\", \" \".join(clone_cmd).replace(clone_url, masked_url))\n",
    "# Use cwd parameter to ensure git runs from a valid directory\n",
    "result = subprocess.run(clone_cmd, capture_output=True, text=True, cwd='/content')\n",
    "if result.returncode != 0:\n",
    "    print(\"git clone stdout:\\n\", result.stdout)\n",
    "    print(\"git clone stderr:\\n\", result.stderr)\n",
    "    sys.exit(\"git clone failed; check REPO_URL, token (if private), and permissions\")\n",
    "\n",
    "os.chdir('/content/Snake')\n",
    "\n",
    "# Install deps (CUDA wheels on Colab are handled automatically by torch)\n",
    "%pip install -q pygame numpy tqdm\n",
    "\n",
    "print('CWD:', os.getcwd())\n",
    "print('Repository cloned and ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Imports ok\n"
     ]
    }
   ],
   "source": [
    "# Quick import test\n",
    "from train import train\n",
    "from types import SimpleNamespace\n",
    "print('Imports ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyperparameters\n",
    "args = SimpleNamespace(\n",
    "    episodes=30000,\n",
    "    max_steps=1000,\n",
    "    buffer_size=200000,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    lr=2.5e-4,\n",
    "    eps_start=1.0,\n",
    "    eps_end=0.1,\n",
    "    eps_decay=120000,\n",
    "    target_update=500,\n",
    "    warmup=20000,\n",
    "    grid=(20, 20),\n",
    "    seed=42,\n",
    "    save_path=\"models/dqn_snake_colab.pt\",\n",
    "    resume=None,  # set to a checkpoint path to continue training\n",
    "    device=\"auto\",  # use GPU if available, else CPU\n",
    "    grad_clip=1.0,  # gradient clipping for stability; set <=0 to disable\n",
    "    double_dqn=True,  # Double DQN for stability\n",
    "    dueling=True,  # Dueling heads for better value/advantage separation\n",
    "    per_alpha=0.4,  # Prioritized replay alpha (lower reduces overfitting to noisy TD errors)\n",
    "    per_beta=0.2,   # PER beta; increase later if desired\n",
    "    eval_every=500,  # run greedy eval every N episodes (0 disables)\n",
    "    eval_episodes=10,  # number of greedy episodes per eval\n",
    "    observation_type=\"image\",  # \"features\" (11 booleans) or \"image\" (CNN input - RECOMMENDED)\n",
    "    n_step=1,  # standard 1-step; more stable early on\n",
    "    save_every=200,  # periodic checkpointing for resume\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA A100-SXM4-80GB\n",
      "Episode 1/500 Reward: -11.60 Epsilon: 0.599 Best: -11.60\n",
      "Episode 10/500 Reward: -14.60 Epsilon: 0.580 Best: -11.30\n",
      "Episode 20/500 Reward: -12.40 Epsilon: 0.559 Best: -4.20\n",
      "Episode 30/500 Reward: -11.30 Epsilon: 0.537 Best: -4.20\n",
      "Episode 40/500 Reward: -11.20 Epsilon: 0.524 Best: -4.20\n",
      "Episode 50/500 Reward: -13.10 Epsilon: 0.508 Best: -4.20\n",
      "Episode 60/500 Reward: -13.80 Epsilon: 0.486 Best: -4.20\n",
      "Episode 70/500 Reward: -13.60 Epsilon: 0.473 Best: -4.20\n",
      "Episode 80/500 Reward: -13.80 Epsilon: 0.454 Best: -4.20\n",
      "Episode 90/500 Reward: -11.90 Epsilon: 0.440 Best: -4.20\n",
      "[Eval @ episode 100] mean: -10.97 median: -10.90 max: -10.90 std: 0.15\n",
      "Episode 100/500 Reward: -15.70 Epsilon: 0.425 Best: -4.20\n",
      "Episode 110/500 Reward: -12.60 Epsilon: 0.415 Best: -4.20\n",
      "Episode 120/500 Reward: -13.60 Epsilon: 0.400 Best: -4.20\n",
      "Episode 130/500 Reward: -11.10 Epsilon: 0.390 Best: -1.30\n",
      "Episode 140/500 Reward: -15.20 Epsilon: 0.378 Best: -1.30\n",
      "Episode 150/500 Reward: -0.90 Epsilon: 0.369 Best: -0.90\n",
      "Episode 160/500 Reward: -11.70 Epsilon: 0.357 Best: -0.90\n",
      "Episode 170/500 Reward: -11.40 Epsilon: 0.348 Best: -0.90\n",
      "Episode 180/500 Reward: -12.50 Epsilon: 0.341 Best: -0.90\n",
      "Episode 190/500 Reward: -3.10 Epsilon: 0.333 Best: -0.90\n",
      "[Eval @ episode 200] mean: -10.97 median: -10.90 max: -10.90 std: 0.15\n",
      "Episode 200/500 Reward: -10.90 Epsilon: 0.325 Best: -0.90\n",
      "Episode 210/500 Reward: -12.50 Epsilon: 0.315 Best: -0.90\n",
      "Episode 220/500 Reward: -11.50 Epsilon: 0.305 Best: 7.00\n",
      "Episode 230/500 Reward: -16.00 Epsilon: 0.290 Best: 7.00\n",
      "Episode 240/500 Reward: -28.80 Epsilon: 0.271 Best: 7.00\n",
      "Episode 250/500 Reward: 32.50 Epsilon: 0.250 Best: 32.50\n",
      "Episode 260/500 Reward: -2.50 Epsilon: 0.231 Best: 74.30\n",
      "Episode 270/500 Reward: 14.50 Epsilon: 0.215 Best: 74.30\n",
      "Episode 280/500 Reward: 11.60 Epsilon: 0.194 Best: 74.80\n",
      "Episode 290/500 Reward: -2.70 Epsilon: 0.171 Best: 96.80\n",
      "[Eval @ episode 300] mean: 266.04 median: 278.85 max: 381.90 std: 64.11\n",
      "Episode 300/500 Reward: -2.00 Epsilon: 0.157 Best: 130.60\n",
      "Episode 310/500 Reward: 6.70 Epsilon: 0.144 Best: 130.60\n",
      "Episode 320/500 Reward: 33.40 Epsilon: 0.132 Best: 130.60\n",
      "Episode 330/500 Reward: -1.40 Epsilon: 0.121 Best: 130.60\n",
      "Episode 340/500 Reward: 108.60 Epsilon: 0.104 Best: 130.60\n",
      "Episode 350/500 Reward: 85.60 Epsilon: 0.094 Best: 130.60\n",
      "Episode 360/500 Reward: 51.00 Epsilon: 0.078 Best: 251.70\n",
      "Episode 370/500 Reward: 52.90 Epsilon: 0.064 Best: 281.40\n",
      "Episode 380/500 Reward: 90.90 Epsilon: 0.054 Best: 281.40\n",
      "Episode 390/500 Reward: 127.00 Epsilon: 0.044 Best: 295.70\n",
      "[Eval @ episode 400] mean: 263.63 median: 247.00 max: 431.60 std: 98.37\n",
      "Episode 400/500 Reward: 101.10 Epsilon: 0.036 Best: 295.70\n",
      "Episode 410/500 Reward: 96.90 Epsilon: 0.029 Best: 315.60\n",
      "Episode 420/500 Reward: 219.30 Epsilon: 0.023 Best: 315.60\n",
      "Episode 430/500 Reward: 221.60 Epsilon: 0.019 Best: 366.40\n",
      "Episode 440/500 Reward: 191.20 Epsilon: 0.016 Best: 366.40\n",
      "Episode 450/500 Reward: 49.70 Epsilon: 0.014 Best: 366.40\n",
      "Episode 460/500 Reward: 248.30 Epsilon: 0.013 Best: 366.40\n",
      "Episode 470/500 Reward: 43.40 Epsilon: 0.012 Best: 366.40\n",
      "Episode 480/500 Reward: 202.50 Epsilon: 0.011 Best: 366.40\n",
      "Episode 490/500 Reward: 99.40 Epsilon: 0.011 Best: 366.40\n",
      "[Eval @ episode 500] mean: 219.68 median: 224.75 max: 429.50 std: 98.63\n",
      "Episode 500/500 Reward: 245.70 Epsilon: 0.011 Best: 366.40\n",
      "Training complete. Best episodic reward: 366.40\n",
      "Best eval mean reward: 266.04\n",
      "Model saved to: models/dqn_snake_colab.pt\n",
      "Training done, saved to models/dqn_snake_colab.pt\n"
     ]
    }
   ],
   "source": [
    "# Train with automatic Drive backup\n",
    "# Checkpoints will be saved to both local path and Google Drive every save_every episodes\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Verify Google Drive is mounted\n",
    "drive_mount = \"/content/drive\"\n",
    "if not os.path.exists(drive_mount):\n",
    "    raise RuntimeError(f\"Google Drive not mounted! Please run the first cell to mount Drive.\")\n",
    "\n",
    "# Drive checkpoint path\n",
    "drive_save_path = \"/content/drive/MyDrive/\" + os.path.basename(args.save_path)\n",
    "args.drive_save_path = drive_save_path\n",
    "print(f\"Drive checkpoint path: {drive_save_path}\")\n",
    "\n",
    "# Check for existing checkpoint in Drive to resume from\n",
    "if os.path.exists(drive_save_path):\n",
    "    args.resume = drive_save_path\n",
    "    print(f\"✓ Found existing checkpoint in Drive: {drive_save_path}\")\n",
    "    try:\n",
    "        ckpt = torch.load(drive_save_path, map_location=\"cpu\")\n",
    "        print(\"Last saved episode:\", ckpt.get(\"episode\"))\n",
    "        print(\"Last frame idx:\", ckpt.get(\"frame_idx\"))\n",
    "        print(\"Best score:\", ckpt.get(\"best_score\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read checkpoint info: {e}\")\n",
    "else:\n",
    "    print(\"No existing checkpoint found in Drive. Starting fresh training.\")\n",
    "\n",
    "print(f\"Periodic saves every {args.save_every} episodes (if save_every > 0)\")\n",
    "\n",
    "train(args, drive_save_path=drive_save_path)\n",
    "\n",
    "print(\"Training done, saved to\", args.save_path)\n",
    "print(f\"Final checkpoint also saved to Drive: {drive_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained model file\n",
    "# This will automatically download the .pt file to your local machine\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "model_path = args.save_path\n",
    "if os.path.exists(model_path):\n",
    "    print(f'Downloading {model_path}...')\n",
    "    files.download(model_path)\n",
    "    print('✓ Model downloaded successfully!')\n",
    "    print(f'File saved as: {os.path.basename(model_path)}')\n",
    "else:\n",
    "    print(f'⚠ Warning: Model file not found at {model_path}')\n",
    "    print('Make sure training completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 229.625, 'median': 237.3000030517578, 'max': 378.0, 'std': 76.93509674072266}\n"
     ]
    }
   ],
   "source": [
    "# Manual greedy evaluation of the latest checkpoint\n",
    "import torch\n",
    "from src.dqn import QNetwork, CNNQNetwork\n",
    "from src.env import SnakeEnv\n",
    "from train import evaluate_policy\n",
    "\n",
    "ckpt = torch.load(args.save_path, map_location=args.device)\n",
    "checkpoint_args = ckpt.get(\"args\", {})\n",
    "observation_type = checkpoint_args.get(\"observation_type\", args.observation_type)\n",
    "grid_size = checkpoint_args.get(\"grid\", args.grid)\n",
    "dueling = checkpoint_args.get(\"dueling\", args.dueling)\n",
    "\n",
    "env = SnakeEnv(\n",
    "    grid_size=tuple(grid_size),\n",
    "    render_mode=None,\n",
    "    seed=args.seed + 999,\n",
    "    observation_type=observation_type,\n",
    ")\n",
    "action_dim = len(env.ACTIONS)\n",
    "\n",
    "policy_state = ckpt[\"policy_state_dict\"]\n",
    "\n",
    "# Determine model type from checkpoint\n",
    "if observation_type == \"image\" or any(k.startswith(\"conv.\") for k in policy_state.keys()):\n",
    "    # CNN model\n",
    "    policy_net = CNNQNetwork(grid_size=tuple(grid_size), output_dim=action_dim, dueling=dueling).to(args.device)\n",
    "    print(\"Using CNN model (image observations)\")\n",
    "else:\n",
    "    # MLP model\n",
    "    sample_state = env.reset()\n",
    "    state_dim = sample_state.shape[0]\n",
    "    hidden_size = policy_state[\"feature.0.weight\"].shape[0]\n",
    "    policy_net = QNetwork(state_dim, action_dim, hidden=hidden_size, dueling=dueling).to(args.device)\n",
    "    print(f\"Using MLP model (feature observations, hidden={hidden_size}, dueling={dueling})\")\n",
    "\n",
    "policy_net.load_state_dict(policy_state)\n",
    "\n",
    "mean_r, median_r, max_r, std_r = evaluate_policy(\n",
    "    policy_net, env, episodes=20, device=args.device, max_steps=args.max_steps, observation_type=observation_type\n",
    ")\n",
    "print({\"mean\": mean_r, \"median\": median_r, \"max\": max_r, \"std\": std_r})\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-812524475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Mount Google Drive (you'll need to authorize once)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Copy model to Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Save model to Google Drive (Alternative download method)\n",
    "# Run this cell if direct download didn't work\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive (you'll need to authorize once)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy model to Drive\n",
    "drive_path = '/content/drive/MyDrive/dqn_snake_colab.pt'\n",
    "shutil.copy(args.save_path, drive_path)\n",
    "print(f'✓ Model saved to Google Drive: {drive_path}')\n",
    "print('Now download it from Google Drive to your local machine!')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
